"""
파일: models.py
설명: 수어 번역을 위한 Uni-Sign 모델 아키텍처 정의.
      ST-GCN 기반 포즈 특징 추출 및 Gemma 3 + LoRA 기반 텍스트 생성.

작성자: 김도완 <dowan.test@gamail.com>
생성일: 2025-04-15
최종 수정일: 2025-04-15
버전: 1.0.0

변경 내역:
- 2025-04-15: MT5 디코더를 Gemma 3 (1b-it)로 교체하고 PEFT LoRA 통합 (김도완)
"""

import torch.nn.functional as F
from torch import Tensor
import torch
from torch import nn
import torch.utils.checkpoint
import contextlib
import torchvision
from einops import rearrange

import math
from stgcn_layers import Graph, get_stgcn_chain
from deformable_attention_2d import DeformableAttention2D
import warnings
from config import mt5_path

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.models.gemma3 import Gemma3ForCausalLM
from peft import LoraConfig, get_peft_model, TaskType

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class Uni_Sign(nn.Module):
    def __init__(self, args):
        super(Uni_Sign, self).__init__()
        self.args = args
        
        self.modes = ['body', 'left', 'right', 'face_all']
        
        self.graph, A = {}, []
        # project (x,y,score) to hidden dim
        hidden_dim = args.hidden_dim
        self.proj_linear = nn.ModuleDict()
        for mode in self.modes:
            self.graph[mode] = Graph(layout=f'{mode}', strategy='distance', max_hop=1)
            A.append(torch.tensor(self.graph[mode].A, dtype=torch.float32, requires_grad=False))
            self.proj_linear[mode] = nn.Linear(3, 64)

        self.gcn_modules = nn.ModuleDict()
        self.fusion_gcn_modules = nn.ModuleDict()
        spatial_kernel_size = A[0].size(0)
        for index, mode in enumerate(self.modes):
            self.gcn_modules[mode], final_dim = get_stgcn_chain(64, 'spatial', (1, spatial_kernel_size), A[index].clone(), True)
            self.fusion_gcn_modules[mode], _ = get_stgcn_chain(final_dim, 'temporal', (5, spatial_kernel_size), A[index].clone(), True)
        
        self.gcn_modules['left'] = self.gcn_modules['right']
        self.fusion_gcn_modules['left'] = self.fusion_gcn_modules['right']
        self.proj_linear['left'] = self.proj_linear['right']

        self.part_para = nn.Parameter(torch.zeros(hidden_dim*len(self.modes)))
        # self.pose_proj = nn.Linear(256*4, 1152)    # gemma3 임베딩 차원이 1152
        
        self.apply(self._init_weights)
        
        if "CSL" in self.args.dataset:
            self.lang = 'Chinese'
        else:
            self.lang = 'English'
        
        if self.args.rgb_support:
            self.rgb_support_backbone = torch.nn.Sequential(*list(torchvision.models.efficientnet_b0(pretrained=True).children())[:-2])
            self.rgb_proj = nn.Conv2d(1280, hidden_dim, kernel_size=1)

            self.fusion_pose_rgb_linear = nn.Linear(hidden_dim, hidden_dim)
            
            # PGF
            self.fusion_pose_rgb_DA = DeformableAttention2D(
                                        dim = hidden_dim,            # feature dimensions
                                        dim_head = 32,               # dimension per head
                                        heads = 8,                   # attention heads
                                        dropout = 0.,                # dropout
                                        downsample_factor = 1,       # downsample factor (r in paper)
                                        offset_scale = None,         # scale of offset, maximum offset
                                        offset_groups = None,        # number of offset groups, should be multiple of heads
                                        offset_kernel_size = 1,      # offset kernel size
                                    )
            
            self.fusion_gate = nn.Sequential(nn.Conv1d(hidden_dim*2, hidden_dim, 1),
                                        nn.GELU(),
                                        nn.Conv1d(hidden_dim, 1, 1),
                                        nn.Tanh(),
                                        nn.ReLU(),
                                    )
            
            for layer in self.fusion_gate:
                if isinstance(layer, nn.Conv1d):
                    nn.init.constant_(layer.weight, 0)
                    nn.init.constant_(layer.bias, 0)

        # Gemma 3 모델 및 토크나이저 로드 (MT5 -> Gemma 3)
        gemma_path = "google/gemma-3-1b-it"    # "google/gemma-3-1b-it"
        # 먼저 토크나이저 로드 및 스페셜 토큰 추가
        self.gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_path, legacy=False, attn_implementation='eager')
        # --- Special Token 추가 ---
        special_tokens_dict = {'additional_special_tokens': ['<visual_start>', '<visual_end>']}
        num_added_toks = self.gemma_tokenizer.add_special_tokens(special_tokens_dict)
        self.visual_start_token_id = self.gemma_tokenizer.convert_tokens_to_ids('<visual_start>')
        self.visual_end_token_id = self.gemma_tokenizer.convert_tokens_to_ids('<visual_end>')
        print(f"Added {num_added_toks} special tokens. Start: {self.visual_start_token_id}, End: {self.visual_end_token_id}")
        # --- Special Token 추가 끝 ---

        self.gemma_model = Gemma3ForCausalLM.from_pretrained(gemma_path, attn_implementation='eager')
        # --- 모델 임베딩 크기 조정 ---
        self.gemma_model.resize_token_embeddings(len(self.gemma_tokenizer))
        # --- 모델 임베딩 크기 조정 끝 ---

         # Gemma 임베딩 차원 가져오기 (resize 후 확인)
        gemma_hidden_dim = self.gemma_model.config.hidden_size # 예: 1152
        visual_feature_dim = 256*4
        self.pose_proj = nn.Linear(visual_feature_dim, gemma_hidden_dim) # Gemma 차원

        # LoRA 설정
        # "gate_proj", "up_proj", "down_proj", "lm_head", "embed_tokens"
        lora_config = LoraConfig(
            r=64,  # Low-rank approximation의 rank (작을수록 메모리와 계산 비용이 적음)
            lora_alpha=128,  # LoRA scaling factor
            lora_dropout=0.1,  # Dropout 비율
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj",
                           "gate_proj", "up_proj", "down_proj",
                           "embed_tokens"],  # LoRA를 적용할 레이어들 (여기서는 attention의 query, value projection에 적용)
            bias="none",  # bias를 포함시킬지 여부 lora_only, none
            task_type=TaskType.CAUSAL_LM
        )

        # LoRA 모델로 변환
        self.lora_model = get_peft_model(self.gemma_model, lora_config)

        # LoRA 모델을 GPU로 이동
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.lora_model.to(self.device)
    
        
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def maybe_autocast(self, dtype=torch.float32):
        # if on cpu, don't use autocast
        # if on gpu, use autocast with dtype if provided, otherwise use torch.float16
        # enable_autocast = self.device != torch.device("cpu")
        enable_autocast = True

        if enable_autocast:
            return torch.cuda.amp.autocast(dtype=dtype)
        else:
            return contextlib.nullcontext()

    def gather_feat_pose_rgb(self, gcn_feat, rgb_feat, indices, rgb_len, pose_init):
        b, c, T, n = gcn_feat.shape
        assert rgb_feat.shape[0] == indices.shape[0]
        rgb_feat = self.rgb_proj(rgb_feat)
        
        assert len(rgb_len) == b
        start = 0
        for batch in range(b):
            index = indices[start:start + rgb_len[batch]].to(torch.long)
            # ignore some invalid rgb clip
            if rgb_len[batch] == 1 and -1 in index:
                start = start + rgb_len[batch]
                continue
            
            # index selection
            gcn_feat_selected = gcn_feat[batch, :, index]
            rgb_feat_selected = rgb_feat[start:start + rgb_len[batch]]
            pose_init_selected = pose_init[start:start + rgb_len[batch]]
            
            gcn_feat_selected = rearrange(gcn_feat_selected, 'c t n -> t c n')
            pose_init_selected = rearrange(pose_init_selected, 't n c -> t c n')
            
            # PGF forward
            with self.maybe_autocast():
                fused_transposed = self.fusion_pose_rgb_DA(pose_feat=gcn_feat_selected,
                                                            rgb_feat=rgb_feat_selected, 
                                                            pose_init=pose_init_selected, )
            
            fused_transposed = fused_transposed.to(gcn_feat.dtype)
            gate_feature = torch.concat([fused_transposed, gcn_feat_selected,], dim=-2)
            gate_score = self.fusion_gate(gate_feature)
            fused_transposed_post = (gate_score) * fused_transposed + (1 - gate_score) * gcn_feat_selected
            
            gcn_feat = gcn_feat.clone() 
            fused_transposed_post = rearrange(fused_transposed_post, 't c n -> c t n')
            
            # replace gcn feature
            gcn_feat[batch, :, index] = fused_transposed_post
            start = start + rgb_len[batch]
            
        assert start == rgb_feat.shape[0]
        return gcn_feat

    def forward(self, src_input, tgt_input):
        # RGB branch forward
        if self.args.rgb_support:
            rgb_support_dict = {}
            for index_key, rgb_key in zip(['left_sampled_indices', 'right_sampled_indices'], ['left_hands', 'right_hands']):
                rgb_feat = self.rgb_support_backbone(src_input[rgb_key])
                
                rgb_support_dict[index_key] = src_input[index_key]
                rgb_support_dict[rgb_key] = rgb_feat
        
        # Pose branch forward
        features = []

        body_feat = None
        for part in self.modes:
            # project position to hidden dim
            proj_feat = self.proj_linear[part](src_input[part]).permute(0,3,1,2) #B,C,T,V
            # spatial gcn forward
            gcn_feat = self.gcn_modules[part](proj_feat)
            if part == 'body':
                body_feat = gcn_feat

            else:
                assert not body_feat is None
                if part == 'left':
                    # Pose RGB fusion
                    if self.args.rgb_support:
                        gcn_feat = self.gather_feat_pose_rgb(gcn_feat, 
                                                            rgb_support_dict[f'{part}_hands'], 
                                                            rgb_support_dict[f'{part}_sampled_indices'], 
                                                            src_input[f'{part}_rgb_len'],
                                                            src_input[f'{part}_skeletons_norm'],
                                                            )
                        
                    gcn_feat = gcn_feat + body_feat[..., -2][...,None].detach()
                    
                elif part == 'right':
                    # Pose RGB fusion
                    if self.args.rgb_support:
                        gcn_feat = self.gather_feat_pose_rgb(gcn_feat, 
                                                                rgb_support_dict[f'{part}_hands'], 
                                                                rgb_support_dict[f'{part}_sampled_indices'],
                                                                src_input[f'{part}_rgb_len'],
                                                                src_input[f'{part}_skeletons_norm'],
                                                                )
                        
                    gcn_feat = gcn_feat + body_feat[..., -1][...,None].detach()

                elif part == 'face_all':
                    gcn_feat = gcn_feat + body_feat[..., 0][...,None].detach()

                else:
                    raise NotImplementedError
            
            # temporal gcn forward
            gcn_feat = self.fusion_gcn_modules[part](gcn_feat) #B,C,T,V
            pool_feat = gcn_feat.mean(-1).transpose(1,2) #B,T,C
            features.append(pool_feat)
        
        # concat sub-pose feature across token dimension
        visual_features = torch.cat(features, dim=-1) + self.part_para

        # Transformer Encoder 미사용
        visual_embeds = self.pose_proj(visual_features) # (B, T_video, Hidden_dim)
        # visual_attention_mask = torch.ones(visual_features.shape[:2], dtype=torch.long, device=visual_features.device)
        visual_attention_mask = src_input['attention_mask'].to(visual_embeds.device)

        
        #------------------------- ADDED [JH] 2025-05-27 ------------------------------------------------------------#
        # Prefix 토큰화 및 임베딩
        if len(src_input['background'])==0: #original translation case
            prefix_text = [f"Translate sign language video to {self.lang}: "] * visual_embeds.shape[0] # 배치 크기 기준
        else: #QA case
            src_prefix = list()
            src_postfix = list()
            for cur_bg in src_input['background']:
                text1 = (f"<bos><start_of_turn>user\n",
                        f"Based on background, reply following conversation\n\n", #real prompt
                        f"Background: {cur_bg}\n",
                        f"User A: ",
                        )
                text2 = (f"User B: <end_of_turn>\n",
                         "<start_of_turn>model\n")
                
                src_prefix.append("".join(text1))
                src_postfix.append("".join(text2))                
            prefix_text = src_prefix #for consistency
            postfix_text =src_postfix
            #prefix_text = [f"Translate sign language video to {self.lang}: "] * visual_embeds.shape[0] # 배치 크기 기준
        #------------------------------------------------------------------------------------------------------------#
        
                
        prefix_token = self.gemma_tokenizer(
            prefix_text, padding="longest", truncation=True, return_tensors="pt"
        ).to(visual_embeds.device)
        prefix_embeds = self.lora_model.get_base_model().model.embed_tokens(prefix_token["input_ids"]) # (B, T_prefix, Hidden_dim)

        #-------------------------------------------------------------#
        if len(src_input['background'])!=0:
            postfix_token = self.gemma_tokenizer(
                postfix_text, padding="longest", truncation=True, return_tensors="pt"
            ).to(visual_embeds.device)
            postfix_embeds = self.lora_model.get_base_model().model.embed_tokens(postfix_token["input_ids"]) # (B, T_prefix, Hidden_dim)        
        #-------------------------------------------------------------#

        # Target 토큰화 (Loss 계산용 labels 생성에 필요)
        eos_token = self.gemma_tokenizer.eos_token
        # bos_token = self.gemma_tokenizer.bos_token # Gemma는 보통 BOS 토큰 사용
        tgt_input['gt_sentence'] = [s + eos_token for s in tgt_input['gt_sentence']]    # <eos> 토큰 추가

        # add_special_tokens로 <bos> 토큰 제외 (<bos>토큰은 prefix에만 필요), padding=left로 해야 <eos>토큰이 입력 데이터에서 제외
        tgt_input_tokenizer = self.gemma_tokenizer(
            tgt_input['gt_sentence'], return_tensors="pt", padding="longest", truncation=True, padding_side='left', add_special_tokens=False, max_length=50
        ).to(visual_embeds.device)

        tgt_input_ids = tgt_input_tokenizer['input_ids']
        tgt_attention_mask = tgt_input_tokenizer['attention_mask']

        # Target 임베딩 생성 (마찬가지로 LoRA base model 사용)
        tgt_embeds = self.lora_model.get_base_model().model.embed_tokens(tgt_input_ids) # (B, T_target, Hidden_dim)

        # --- Special Token 임베딩 생성 ---
        batch_size = visual_embeds.shape[0]
        visual_start_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
            torch.tensor([self.visual_start_token_id] * batch_size, device=visual_embeds.device)
        ).unsqueeze(1) # (B, 1, Hidden_dim)
        visual_end_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
            torch.tensor([self.visual_end_token_id] * batch_size, device=visual_embeds.device)
        ).unsqueeze(1) # (B, 1, Hidden_dim)
        # --- Special Token 임베딩 생성 끝 ---

        # 모델 입력 임베딩: prefix + <visual_start> + visual + <visual_end> + target (shifted right)
        if len(src_input['background'])==0:
            training_inputs_embeds = torch.cat([
                prefix_embeds,              # (B, T_prefix, H)
                visual_start_token_embeds,  # (B, 1,       H)
                visual_embeds,              # (B, T_video, H)
                visual_end_token_embeds,    # (B, 1,       H)
                tgt_embeds[:, :-1, :]       # (B, T_target-1, H) -> target의 마지막 토큰 제외(<eos> 토큰)
            ], dim=1)
        else:
            training_inputs_embeds = torch.cat([
                prefix_embeds,              # (B, T_prefix, H)
                visual_start_token_embeds,  # (B, 1,       H)
                visual_embeds,              # (B, T_video, H)
                visual_end_token_embeds,    # (B, 1,       H)
                postfix_embeds,
                tgt_embeds[:, :-1, :]       # (B, T_target-1, H) -> target의 마지막 토큰 제외(<eos> 토큰)
            ], dim=1)
            
        # Attention Mask 생성: Special token 자리에도 1 추가
        visual_start_mask = torch.ones(batch_size, 1, dtype=torch.long, device=visual_embeds.device)
        visual_end_mask = torch.ones(batch_size, 1, dtype=torch.long, device=visual_embeds.device)

        if len(src_input['background'])==0:
            training_attention_mask = torch.cat([
                prefix_token['attention_mask'], # (B, T_prefix)
                visual_start_mask,              # (B, 1)
                visual_attention_mask,          # (B, T_video) - 원본 비디오 길이 반영
                visual_end_mask,                # (B, 1)
                tgt_attention_mask[:, :-1]      # (B, T_target-1)
            ], dim=1)
        else:
            training_attention_mask = torch.cat([
                prefix_token['attention_mask'], # (B, T_prefix)
                visual_start_mask,              # (B, 1)
                visual_attention_mask,          # (B, T_video) - 원본 비디오 길이 반영
                visual_end_mask,                # (B, 1)
                postfix_token['attention_mask'],
                tgt_attention_mask[:, :-1]      # (B, T_target-1)
            ], dim=1)
            
        # 모델 호출 (LoRA 모델 직접 사용)
        out = self.lora_model(inputs_embeds=training_inputs_embeds,
                              attention_mask=training_attention_mask,
                                output_hidden_states=False, #added
                              return_dict=True)

        all_logits = out.logits # (B, T_prefix + 1 + T_video + 1 + T_target - 1, Vocab_size)

        #----------------------- CL Loss Start ------------------------------#

        alpha = 0.25
        if False: #use CL Loss
            prefix_len  = prefix_embeds.size(1)              # <bos> 제외
            video_len   = visual_embeds.size(1)
            idx_vid_end = prefix_len + 1 + video_len         # <visual_end> 토큰 위치

            # 2) hidden state 추출  → (B, H)
            #h = out.last_hidden_state[:, idx_vid_end, :]         # (B, L, H)  ← 추가
            h = out.hidden_states[-1][:, idx_vid_end, :]     # 마지막 layer 사용

            # 3) 정규화 & 유사도 행렬  S = h · hᵀ / τ
            temperature = 0.07
            h_norm  = F.normalize(h, dim=-1)                 # (B, H)
            logits = h_norm @ h_norm.T                      # (B, B)
            logits = logits / temperature

            # 4) InfoNCE: 자기 자신(대각)만 positive
            batch_size = h.size(0)
            target = torch.arange(batch_size, device=h.device)
            cl_loss = F.cross_entropy(logits, target)
        else:
            cl_loss = 0        
        
        
        #----------------------- CL Loss end -----------------------------------#
        
        
        
        
        
        

        # Loss 계산용 레이블 준비
        labels = tgt_input_ids.clone() # (B, T_target-1)
        labels[tgt_attention_mask == 0] = -100 # 패딩 부분 마스킹

        # Loss 계산용 로짓 선택: Target 예측에 해당하는 부분만 선택
        prefix_len = prefix_embeds.shape[1] - 1   # <bos>길이 제외 출력은 <bos> 안나오니까
        video_len = visual_embeds.shape[1]
        # Target logits 시작 위치: prefix 길이 + <visual-start> + 비디오 길이 + <visual-end>

        if len(src_input['background'])==0:
            start_target_logits_index = prefix_len + 1 + video_len + 1
        else:
            start_target_logits_index = prefix_len + 1 + video_len + postfix_embeds.shape[1] + 1 
            
        # 로짓의 길이는 T_target - 1 이어야 함
        end_target_logits_index = start_target_logits_index + tgt_embeds.shape[1]

        # 전체 로짓 시퀀스 길이와 계산된 end index 비교 (디버깅용)
        total_logit_len = all_logits.shape[1]

        if len(src_input['background'])==0:
            expected_logit_len_before_target = prefix_len + 1 + video_len + 1
        else:
            expected_logit_len_before_target = prefix_len + 1 + video_len + postfix_embeds.shape[1] + 1 
            
        expected_target_logit_len = tgt_embeds.shape[1]
        assert total_logit_len == expected_logit_len_before_target + expected_target_logit_len

        # 로짓 슬라이싱 시 인덱스 범위 확인
        if start_target_logits_index >= total_logit_len:
             # 이 경우 target 예측에 해당하는 로짓이 없음 (길이 계산 오류 가능성)
             print(f"Warning: start_target_logits_index ({start_target_logits_index}) >= total_logit_len ({total_logit_len}). Check lengths.")
             # 임시로 비어있는 텐서 또는 에러 처리
             target_logits = torch.empty(batch_size, 0, all_logits.size(-1), device=all_logits.device)
        else:
            # 정상 슬라이싱, 길이는 T_target - 1 만큼만
            target_logits = all_logits[:, start_target_logits_index : end_target_logits_index, :] # (B, T_target - 1, Vocab_size)

        # Loss 계산
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)

        # 로짓과 레이블의 길이(T_target-1)가 같은지 확인 후 계산
        loss_logit_len = target_logits.shape[1]
        loss_label_len = labels.shape[1]

        if loss_logit_len == 0: # target_logits가 비어있는 경우 처리
             loss = torch.tensor(0.0, device=all_logits.device, requires_grad=True) # 또는 다른 적절한 처리
             print("Warning: No target logits found for loss calculation.")
        elif loss_logit_len != loss_label_len:
            # 길이가 다르면 작은 쪽에 맞춰서 계산 (잠재적 문제 알림)
            print(f"Warning: Mismatch in target logits ({loss_logit_len}) and labels ({loss_label_len}) length for loss. Using min length.")
            min_len_for_loss = min(loss_logit_len, loss_label_len)
            if min_len_for_loss == 0:
                 loss = torch.tensor(0.0, device=all_logits.device, requires_grad=True)
            else:
                 loss = loss_fct(target_logits[:, :min_len_for_loss, :].reshape(-1, target_logits.size(-1)),
                                 labels[:, :min_len_for_loss].reshape(-1).long())
        else:
            # 길이가 같으면 정상 계산
            loss = loss_fct(target_logits.reshape(-1, target_logits.size(-1)),
                            labels.reshape(-1).long())

        stack_out = {
            'inputs_embeds': training_inputs_embeds,
            'attention_mask': training_attention_mask,
            'loss': loss + alpha * cl_loss,
            # 'target_logits': target_logits, # Target 예측에 해당하는 로짓 슬라이스
            # 'labels': labels              # Loss 계산에 사용된 최종 레이블 (마스킹 적용됨)
        }

        return stack_out

    @torch.no_grad()
    def generate(self,
                 src_input,
                 max_new_tokens,
                 num_beams):
    
        # 수어 영상 특징 추출, 기존과 동일
        features = []
        body_feat = None
        
        rgb_support_dict = {}
        if self.args.rgb_support:
             for index_key, rgb_key in zip(['left_sampled_indices', 'right_sampled_indices'], ['left_hands', 'right_hands']):
                  rgb_feat = self.rgb_support_backbone(src_input[rgb_key])
                  rgb_support_dict[index_key] = src_input[index_key]
                  rgb_support_dict[rgb_key] = rgb_feat
    
        for part in self.modes:
            proj_feat = self.proj_linear[part](src_input[part]).permute(0,3,1,2) #B,C,T,V
            gcn_feat = self.gcn_modules[part](proj_feat)
            if part == 'body':
                body_feat = gcn_feat
            else:
                assert not body_feat is None
                
                if part == 'left':
                     if self.args.rgb_support:
                         gcn_feat = self.gather_feat_pose_rgb(gcn_feat, rgb_support_dict[f'{part}_hands'], rgb_support_dict[f'{part}_sampled_indices'], src_input[f'{part}_rgb_len'], src_input[f'{part}_skeletons_norm'])
                     gcn_feat = gcn_feat + body_feat[..., -2][...,None].detach()
                elif part == 'right':
                     if self.args.rgb_support:
                          gcn_feat = self.gather_feat_pose_rgb(gcn_feat, rgb_support_dict[f'{part}_hands'], rgb_support_dict[f'{part}_sampled_indices'], src_input[f'{part}_rgb_len'], src_input[f'{part}_skeletons_norm'])
                     gcn_feat = gcn_feat + body_feat[..., -1][...,None].detach()
                elif part == 'face_all':
                     gcn_feat = gcn_feat + body_feat[..., 0][...,None].detach()
                else: raise NotImplementedError
    
            gcn_feat = self.fusion_gcn_modules[part](gcn_feat) #B,C,T,V
            pool_feat = gcn_feat.mean(-1).transpose(1,2) #B,T,C
            features.append(pool_feat)

            # ───────────────────────────────────────── [GENERATE] 수정본 ───────────────────────────────────────── #
        # concat sub-pose feature -> 최종 Visual Embedding
        visual_features = torch.cat(features, dim=-1) + self.part_para.to(self.device)

        # Transformer Encoder 미사용
        visual_embeds = self.pose_proj(visual_features)                                       # (B, T_video, H)
        visual_attention_mask = src_input['attention_mask'].to(visual_embeds.device)          # (B, T_video)

        # ------------------------- ADDED [JH] 2025-05-27 ----------------------------------------------- #
        # 1. Prefix / Post-fix 구성 (번역 ↔ QA 구분)
        batch_size = visual_embeds.shape[0]
        if len(src_input['background']) == 0:             # (A) 순수 번역
            prefix_text  = [f"Translate sign language video to {self.lang}: "] * batch_size
            use_postfix  = False
        else:                                             # (B) 대화-기반 QA
            src_prefix, src_postfix = [], []
            for cur_bg in src_input['background']:
                text1 = (
                    "<bos><start_of_turn>user\n"
                    "Based on background, reply following conversation\n\n"
                    f"Background: {cur_bg}\n"
                    "User A: "
                )
                text2 = (
                    "User B: <end_of_turn>\n"
                    "<start_of_turn>model\n"
                )
                src_prefix.append(text1)
                src_postfix.append(text2)
            prefix_text  = src_prefix
            postfix_text = src_postfix
            use_postfix  = True
        # ----------------------------------------------------------------------------------------------- #

        # 2. Prefix 임베딩  
        prefix_token = self.gemma_tokenizer(
            prefix_text, padding="longest", truncation=True, return_tensors="pt"
        ).to(self.device)
        prefix_embeds = self.lora_model.get_base_model().model.embed_tokens(prefix_token["input_ids"])

        # 3. (선택) Post-fix 임베딩
        if use_postfix:
            postfix_token = self.gemma_tokenizer(
                postfix_text, padding="longest", truncation=True, return_tensors="pt"
            ).to(self.device)
            postfix_embeds = self.lora_model.get_base_model().model.embed_tokens(postfix_token["input_ids"])

        # 4. Special Token 임베딩
        visual_start_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
            torch.tensor([self.visual_start_token_id] * batch_size, device=self.device)
        ).unsqueeze(1)
        visual_end_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
            torch.tensor([self.visual_end_token_id] * batch_size, device=self.device)
        ).unsqueeze(1)

        # 5. Inference 입력 임베딩
        if use_postfix:
            inference_inputs_embeds = torch.cat([
                prefix_embeds,
                visual_start_token_embeds,
                visual_embeds,
                visual_end_token_embeds,
                postfix_embeds
            ], dim=1)
        else:
            inference_inputs_embeds = torch.cat([
                prefix_embeds,
                visual_start_token_embeds,
                visual_embeds,
                visual_end_token_embeds
            ], dim=1)

        # 6. Inference Attention Mask
        visual_start_mask = torch.ones(batch_size, 1, dtype=torch.long, device=self.device)
        visual_end_mask   = torch.ones(batch_size, 1, dtype=torch.long, device=self.device)

        mask_parts = [
            prefix_token['attention_mask'],          # (B, T_prefix)
            visual_start_mask,                       # (B, 1)
            visual_attention_mask,                   # (B, T_video)
            visual_end_mask                          # (B, 1)
        ]
        if use_postfix:
            mask_parts.append(postfix_token['attention_mask'])  # (B, T_postfix)

        inference_attention_mask = torch.cat(mask_parts, dim=1) # (B, T_total)
    #     # concat sub-pose feature -> 최종 Visual Embedding
    #     visual_features = torch.cat(features, dim=-1) + self.part_para.to(self.device) # param도 device 통일

    #     # Transformer Encoder 미사용
    #     visual_embeds = self.pose_proj(visual_features) # (B, T_video, Gemma_hidden_dim)
    #     # visual_attention_mask = torch.ones(visual_features.shape[:2], dtype=torch.long, device=visual_features.device)    # 수어 특징 벡터 전체 사용
    #     visual_attention_mask = src_input['attention_mask'].to(visual_embeds.device)    # 원본 비디오 길이 만큼의 attention mask

    #    # 2. Prefix Embedding 계산
    #     batch_size = visual_embeds.shape[0]
    #     prefix_text = [f"Translate sign language video to {self.lang}: "] * batch_size
    #     # prefix_text = [f"Consider the hand shapes, movements, and facial expressions in the sign language video features. Thinking step-by-step, synthesize this information and translate into {self.lang}:"] * batch_size
    #     prefix_token = self.gemma_tokenizer(
    #         prefix_text, padding="longest", truncation=True, return_tensors="pt"
    #     ).to(self.device) # device 통일
    #     prefix_embeds = self.lora_model.get_base_model().model.embed_tokens(prefix_token["input_ids"])

    #     # 3. Special Token 임베딩 생성
    #     visual_start_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
    #         torch.tensor([self.visual_start_token_id] * batch_size, device=self.device)
    #     ).unsqueeze(1)
    #     visual_end_token_embeds = self.lora_model.get_base_model().model.embed_tokens(
    #         torch.tensor([self.visual_end_token_id] * batch_size, device=self.device)
    #     ).unsqueeze(1)

    #     # 4. Inference 입력 임베딩: prefix + <visual_start> + visual + <visual_end>
    #     inference_inputs_embeds = torch.cat([
    #         prefix_embeds,
    #         visual_start_token_embeds,
    #         visual_embeds,
    #         visual_end_token_embeds
    #     ], dim=1)

    #     # 5. Inference Attention Mask
    #     visual_start_mask = torch.ones(batch_size, 1, dtype=torch.long, device=self.device)
    #     visual_end_mask = torch.ones(batch_size, 1, dtype=torch.long, device=self.device)
    #     inference_attention_mask = torch.cat([
    #         prefix_token['attention_mask'],
    #         visual_start_mask,
    #         visual_attention_mask, # 원본 비디오 길이 마스크
    #         visual_end_mask
    #     ], dim=1)

    #     # inference_attention_mask = src_input['attention_mask']
    #     # inference_inputs_embeds = src_input['inputs_embeds']

        # generate 호출
        outputs = self.lora_model.generate(
            inputs_embeds=inference_inputs_embeds,
            attention_mask=inference_attention_mask,
            max_new_tokens=max_new_tokens,
            num_beams=num_beams,
            pad_token_id=self.gemma_tokenizer.pad_token_id,
            eos_token_id=self.gemma_tokenizer.eos_token_id,
            min_length=1,  # 최소 1 토큰 생성
            # do_sample=False,
            # top_k=50,      # 상위 50개 토큰 중 샘플링
            # top_p=0.95,    # 누적 확률 95% 내에서 샘플링
        )
        return outputs

def get_requires_grad_dict(model):
    param_requires_grad = {name: True for name, param in model.named_parameters()}
    param_requires_grad_right = {}
    for key in param_requires_grad.keys():
        if 'left' in key:
            param_requires_grad_right[key.replace("left", 'right')] = param_requires_grad[key]
    param_requires_grad = {**param_requires_grad,
                           **param_requires_grad_right}
    params_to_update = {k: v for k, v in model.state_dict().items() if param_requires_grad.get(k, True)}

    return params_to_update



#CE Loss 이전에 debugging code
        # # --- Debugging: 예측값과 정답 레이블 비교 확인 ---
        # # 특정 조건에서만 출력 (예: global_step % 100 == 0)
        # # global_step = ... # 학습 루프에서 전달 받거나 계산 필요 
        # # debug_this_step = (global_step % 100 == 0)
        # debug_this_step = True # 임시로 항상 출력 (디버깅 완료 후 False 또는 조건부 변경)
        # num_examples_to_log = 2 # 로그를 출력할 배치 내 샘플 수
        
        # if debug_this_step and target_logits.shape[0] > 0: # 배치 크기 0 이상일 때
        #     try:
        #         print("\n" + "="*70)
        #         print(f" 예측 vs 정답 비교 (현재 배치) ")
        #         print("="*70)
        
        #         # 모델 예측값 중 가장 확률이 높은 토큰 ID 얻기
        #         # target_logits shape: (Batch, SeqLen, VocabSize)
        #         predicted_ids = torch.argmax(target_logits, dim=-1) # Shape: (Batch, SeqLen)
        
        #         # 지정된 수의 샘플에 대해 반복
        #         for i in range(min(num_examples_to_log, target_logits.shape[0])):
        #             print(f"--- 배치 내 샘플 인덱스: {i} ---")
        #             # 현재 샘플의 레이블과 예측 ID 가져오기
        #             example_labels = labels[i].cpu().tolist()
        #             example_preds = predicted_ids[i].cpu().tolist()
        #             seq_len = len(example_labels) # 레이블 시퀀스 길이
        
        #             # 보기 좋게 테이블 형태로 출력
        #             print(f"{'위치':<4} | {'정답 ID':<8} | {'정답 토큰':<12} | {'예측 ID':<8} | {'예측 토큰':<12} | {'일치?':<5}")
        #             print(f"------|----------|--------------|----------|--------------|-------")
        
        #             # 시퀀스의 각 스텝별로 비교
        #             for step in range(seq_len):
        #                 label_id = example_labels[step]
        #                 pred_id = example_preds[step]
        
        #                 # 패딩(-100)이 아닌 경우에만 상세 정보 출력
        #                 if label_id != -100:
        #                     # 토큰 ID를 실제 문자열로 디코딩 (특수 토큰 포함)
        #                     label_token = self.gemma_tokenizer.decode(label_id, skip_special_tokens=False)
        #                     pred_token = self.gemma_tokenizer.decode(pred_id, skip_special_tokens=False)
        #                     is_correct = "Yes" if label_id == pred_id else "No"
        #                     # 각 컬럼 너비에 맞춰 출력
        #                     print(f"{step:<4} | {label_id:<8} | {label_token:<12} | {pred_id:<8} | {pred_token:<12} | {is_correct:<5}")
        #                # else: # 필요하다면 패딩(-100) 위치도 출력
        #               #     pred_token = self.gemma_tokenizer.decode(pred_id, skip_special_tokens=False)
        #                #     print(f"{step:<4} | {'-100':<8} | {'PAD':<12} | {pred_id:<8} | {pred_token:<12} | {'-':<5}")
        
        #             print("-" * 70) # 샘플 구분선
        
        #     except Exception as e:
        #         print(f"\n[!!! 에러 발생 !!!] 예측/정답 비교 중 에러: {e}\n")
        
        # # --- Debugging End ---
